{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train models",
   "id": "2c02a37b33d93304"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T12:02:20.518739Z",
     "start_time": "2025-11-19T12:02:20.512150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#file paths\n",
    "notebook_dir = Path.cwd()\n",
    "project_dir = notebook_dir.parent\n",
    "train_path = project_dir / 'data' / 'processed' / 'train_processed.csv'\n",
    "models_dir = project_dir / 'models'"
   ],
   "id": "fa316aa6f4cd3606",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare data for model training",
   "id": "51b8fda632cad49b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:53:48.544519Z",
     "start_time": "2025-11-19T16:53:48.517084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#read data\n",
    "train = pd.read_csv(train_path)\n",
    "\n",
    "#define the features\n",
    "feature_cols = ['Sex_numeric','Age', 'Pclass','Fare','FarePerPerson','HasCabin',\n",
    "                #Family size\n",
    "                'Alone','Small family','Large family',\n",
    "                #Title\n",
    "                'Title__Master','Title__Miss', 'Title__Mr', 'Title__Mrs',\n",
    "                #Embarked\n",
    "                'Embarked__C', 'Embarked__Q', 'Embarked__S',\n",
    "                #Age Group\n",
    "                'Age_Adult', 'Age_Child', 'Age_Elderly', 'Age_Teen',\n",
    "                #Deck\n",
    "                'Deck_A', 'Deck_B', 'Deck_C',\t'Deck_D', 'Deck_E', 'Deck_F', 'Deck_G', 'Deck_T', 'Deck_Unknown'\n",
    "                ]\n",
    "\n",
    "X = train[feature_cols] #features\n",
    "Y = train['Survived'] #target\n",
    "\n",
    "#train/validation split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X,Y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#data scaling (Standard Scaler)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "#update data after scaling\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "#save scaler\n",
    "with open(models_dir / 'scaler.pkl','wb') as file:\n",
    "    pickle.dump(scaler,file)\n",
    "    \n",
    "#save features\n",
    "with open(models_dir / 'features.pkl','wb') as file:\n",
    "    pickle.dump(feature_cols,file)"
   ],
   "id": "b54cca84822d3fd6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Logistic Regression",
   "id": "3db42fe77c0da412"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#model training - Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=100)\n",
    "lr_model.fit(X_train_scaled,Y_train)\n",
    "\n",
    "#eval\n",
    "lr_predictions = lr_model.predict(X_val_scaled)\n",
    "lr_accuracy = accuracy_score(Y_val, lr_predictions)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.2%}\")\n",
    "\n",
    "#print feature name and wage pairs, print bias\n",
    "feature_weight = sorted(list(zip(feature_cols,lr_model.coef_[0])),key=lambda x: abs(x[1]),reverse=True)\n",
    "for name,wage in feature_weight:\n",
    "    print(f'{name:<15}: {wage:+.3f}')\n",
    "print(\"Bias:\", lr_model.intercept_)\n",
    "\n",
    "#save model\n",
    "with open(models_dir / 'lr_model.pkl','wb') as file:\n",
    "    pickle.dump(lr_model,file)\n"
   ],
   "id": "b9c8b608f6c47ada"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Random Forrest",
   "id": "b00eb7adfc5e3b08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:53:57.317011Z",
     "start_time": "2025-11-19T16:53:53.594785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model training - Random Forrest (no scaling)\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,      # num of trees\n",
    "    max_depth=10,          # max tree depth\n",
    "    min_samples_split=5,   # min samples to split\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train,Y_train)\n",
    "\n",
    "cv_scores = cross_val_score(rf_model, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "print(f\"RF - CV mean: {cv_scores.mean():.2%}, standard deviation: {cv_scores.std():.2%}\")\n",
    "\n",
    "\n",
    "rf_predictions = rf_model.predict(X_val)\n",
    "rf_accuracy = accuracy_score(Y_val,rf_predictions)\n",
    "\n",
    "print(f\"\\nRandom Forrest Accuracy: {rf_accuracy:.2%}\")\n",
    "feature_importance = sorted(list(zip(feature_cols, rf_model.feature_importances_)),\n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "for name, importance in feature_importance:\n",
    "    print(f'{name:<15}: {importance*100:.3f}')\n",
    "\n",
    "#save model\n",
    "with open(models_dir / 'rf_model.pkl','wb') as file:\n",
    "    pickle.dump(rf_model,file)"
   ],
   "id": "e64f9bf6db0bc337",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF - CV mean: 82.86%, standard deviation: 1.41%\n",
      "\n",
      "Random Forrest Accuracy: 83.80%\n",
      "Title__Mr      : 16.663\n",
      "Sex_numeric    : 13.521\n",
      "FarePerPerson  : 11.602\n",
      "Fare           : 10.993\n",
      "Age            : 9.641\n",
      "Pclass         : 5.905\n",
      "Title__Mrs     : 5.575\n",
      "Title__Miss    : 4.050\n",
      "Large family   : 3.146\n",
      "Small family   : 2.757\n",
      "Deck_Unknown   : 2.459\n",
      "HasCabin       : 2.186\n",
      "Embarked__S    : 1.308\n",
      "Age_Child      : 1.306\n",
      "Embarked__C    : 1.303\n",
      "Alone          : 1.270\n",
      "Deck_E         : 0.984\n",
      "Age_Adult      : 0.898\n",
      "Title__Master  : 0.699\n",
      "Deck_B         : 0.688\n",
      "Embarked__Q    : 0.645\n",
      "Deck_C         : 0.605\n",
      "Deck_D         : 0.540\n",
      "Age_Teen       : 0.405\n",
      "Age_Elderly    : 0.292\n",
      "Deck_A         : 0.220\n",
      "Deck_G         : 0.180\n",
      "Deck_F         : 0.139\n",
      "Deck_T         : 0.019\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. XGB Boost",
   "id": "216d30e05722496"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#model training - XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "xgb_predictions = xgb_model.predict(X_val)\n",
    "xgb_accuracy = accuracy_score(Y_val,xgb_predictions)\n",
    "print(xgb_accuracy)\n",
    "\n",
    "#save model\n",
    "with open(models_dir / 'xgb_model.pkl','wb') as file:\n",
    "    pickle.dump(xgb_model,file)"
   ],
   "id": "fcc8bb224d11133c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. MLP from scratch",
   "id": "d03c7a67206a690"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:50:21.171726Z",
     "start_time": "2025-11-19T11:48:21.515704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model training - MLP from scratch\n",
    "import sys\n",
    "sys.path.append(str(models_dir))\n",
    "from models.mlp_from_scratch import Network\n",
    "\n",
    "mlp_model = Network([len(feature_cols),16,1])\n",
    "\n",
    "history = mlp_model.train(\n",
    "    X_train_scaled,\n",
    "    Y_train.values,\n",
    "    epochs=500,\n",
    "    learning_rate=0.001,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "mlp_accuracy = mlp_model.evaluate(X_val_scaled, Y_val.values)\n",
    "print(f\"MLP Accuracy: {mlp_accuracy:.2%}\")\n",
    "\n",
    "mlp_model.save_weights(models_dir / 'mlp_model_16_1.pkl')"
   ],
   "id": "4bdca424fb0d1da8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500, Loss: 0.232246\n",
      "Epoch 10/500, Loss: 0.123486\n",
      "Epoch 20/500, Loss: 0.121964\n",
      "Epoch 30/500, Loss: 0.121099\n",
      "Epoch 40/500, Loss: 0.120425\n",
      "Epoch 50/500, Loss: 0.119829\n",
      "Epoch 60/500, Loss: 0.119254\n",
      "Epoch 70/500, Loss: 0.118670\n",
      "Epoch 80/500, Loss: 0.118057\n",
      "Epoch 90/500, Loss: 0.117390\n",
      "Epoch 100/500, Loss: 0.116638\n",
      "Epoch 110/500, Loss: 0.115818\n",
      "Epoch 120/500, Loss: 0.114979\n",
      "Epoch 130/500, Loss: 0.114146\n",
      "Epoch 140/500, Loss: 0.113319\n",
      "Epoch 150/500, Loss: 0.112519\n",
      "Epoch 160/500, Loss: 0.111757\n",
      "Epoch 170/500, Loss: 0.111043\n",
      "Epoch 180/500, Loss: 0.110375\n",
      "Epoch 190/500, Loss: 0.109752\n",
      "Epoch 200/500, Loss: 0.109173\n",
      "Epoch 210/500, Loss: 0.108636\n",
      "Epoch 220/500, Loss: 0.108136\n",
      "Epoch 230/500, Loss: 0.107670\n",
      "Epoch 240/500, Loss: 0.107232\n",
      "Epoch 250/500, Loss: 0.106809\n",
      "Epoch 260/500, Loss: 0.106367\n",
      "Epoch 270/500, Loss: 0.105920\n",
      "Epoch 280/500, Loss: 0.105472\n",
      "Epoch 290/500, Loss: 0.105024\n",
      "Epoch 300/500, Loss: 0.104575\n",
      "Epoch 310/500, Loss: 0.104137\n",
      "Epoch 320/500, Loss: 0.103706\n",
      "Epoch 330/500, Loss: 0.103274\n",
      "Epoch 340/500, Loss: 0.102842\n",
      "Epoch 350/500, Loss: 0.102404\n",
      "Epoch 360/500, Loss: 0.101960\n",
      "Epoch 370/500, Loss: 0.101533\n",
      "Epoch 380/500, Loss: 0.101120\n",
      "Epoch 390/500, Loss: 0.100716\n",
      "Epoch 400/500, Loss: 0.100314\n",
      "Epoch 410/500, Loss: 0.099915\n",
      "Epoch 420/500, Loss: 0.099528\n",
      "Epoch 430/500, Loss: 0.099148\n",
      "Epoch 440/500, Loss: 0.098773\n",
      "Epoch 450/500, Loss: 0.098405\n",
      "Epoch 460/500, Loss: 0.098046\n",
      "Epoch 470/500, Loss: 0.097695\n",
      "Epoch 480/500, Loss: 0.097352\n",
      "Epoch 490/500, Loss: 0.097018\n",
      "MLP Accuracy: 82.68%\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
